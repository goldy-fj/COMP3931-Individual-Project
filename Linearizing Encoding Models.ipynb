{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07ee83d",
   "metadata": {},
   "source": [
    "# How the Human Brain Makes Sense of Natural Scenes\n",
    "## Linearizing Encoding Model - Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d319d3",
   "metadata": {},
   "source": [
    "#### Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df20a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from scipy.stats import pearsonr as corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'nsd_data'\n",
    "parent_submission_dir = 'submission_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35fbb8",
   "metadata": {},
   "source": [
    "#### Select Subject (1-8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 1 # 1, 2, 3, 4, 5, 6, 7, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e65b15",
   "metadata": {},
   "source": [
    "#### Define data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "    \n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54097bbd",
   "metadata": {},
   "source": [
    "## Load fMRI training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25250e9",
   "metadata": {},
   "source": [
    "Load the training split of the fMRI data (from data_dir) for the selected subject (subj), for both hemispheres (LH/RH).\n",
    "\n",
    "The files each contain a 2-dimensional array of the training stimulus images and corresponding fMRI veriticies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7120896",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)\\n')\n",
    "\n",
    "print('RH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fdb2d2",
   "metadata": {},
   "source": [
    "## fMRI ROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2fe1f",
   "metadata": {},
   "source": [
    "Reigons-of-interest (ROIs) divide the visual cortex into different local groups of vertices, performing similar functions. The files in the 'roi_masks' directory map brain ROIs to vertcies and vertices to fsaverage space, a standardized brain surface map for all subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde3c71",
   "metadata": {},
   "source": [
    "### Visualize all vertices on a brain surface map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8146d",
   "metadata": {},
   "source": [
    "Map all vertices, using the left and right ROI masks provided in the files '\\*h.all-vertices_fsaverage_space.npy', to the indicies on the brain surface map in fsaverage space. We then output the graphical brain surface map highlighting the areas that represent the verticies mapped for all ROIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b30e3",
   "metadata": {},
   "source": [
    "#### Select Hemisphere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dce5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere = 'left' # 'left', right'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cfad9",
   "metadata": {},
   "source": [
    "#### Brain Surface Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67484efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the brain surface map of all vertices\n",
    "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0]+'h.all-vertices_fsaverage_space.npy')\n",
    "fsaverage_all_vertices = np.load(roi_dir)\n",
    "\n",
    "# Create the interactive brain surface map\n",
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "view = plotting.view_surf(\n",
    "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "    surf_map=fsaverage_all_vertices,\n",
    "    bg_map=fsaverage['sulc_'+hemisphere],\n",
    "    threshold=1e-14,\n",
    "    cmap='cool',\n",
    "    colorbar=False,\n",
    "    title='All vertices, '+hemisphere+' hemisphere'\n",
    "    )\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e5830",
   "metadata": {},
   "source": [
    "### Visualize a chosen ROI on a brain surface map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee632e7",
   "metadata": {},
   "source": [
    "Using the files '\\*h.\\*_fsaverage_space.npy' we map the vertices of several ROIs from a given class to the brain surface map in fsaverage space, then use the files mapping_\\*.npy to select a specific ROI within the class to visualize on the brain surface map. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6285cd",
   "metadata": {},
   "source": [
    "#### Select Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere = 'left' # 'left', right'\n",
    "roi = \"EBA\" #\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85324fd3",
   "metadata": {},
   "source": [
    "#### Brain Surface Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f494197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ROI class based on the selected ROI\n",
    "if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
    "    roi_class = 'prf-visualrois'\n",
    "elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
    "    roi_class = 'floc-bodies'\n",
    "elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
    "    roi_class = 'floc-faces'\n",
    "elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
    "    roi_class = 'floc-places'\n",
    "elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
    "    roi_class = 'floc-words'\n",
    "elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
    "    roi_class = 'streams'\n",
    "\n",
    "# Load the ROI brain surface maps\n",
    "roi_class_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0]+'h.'+roi_class+'_fsaverage_space.npy')\n",
    "roi_map_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    'mapping_'+roi_class+'.npy')\n",
    "fsaverage_roi_class = np.load(roi_class_dir)\n",
    "roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
    "\n",
    "# Select the vertices corresponding to the ROI of interest\n",
    "roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
    "fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n",
    "\n",
    "# Create the interactive brain surface map\n",
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "view = plotting.view_surf(\n",
    "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "    surf_map=fsaverage_roi,\n",
    "    bg_map=fsaverage['sulc_'+hemisphere],\n",
    "    threshold=1e-14,\n",
    "    cmap='cool',\n",
    "    colorbar=False,\n",
    "    title=roi+', '+hemisphere+' hemisphere'\n",
    "    )\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893f6f1",
   "metadata": {},
   "source": [
    "## Stimulus images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6680299",
   "metadata": {},
   "source": [
    "Images of natural scenes come from the COCO dataset. https://cocodataset.org/#home\n",
    "\n",
    "The images were pre-split into training and test partitions and correspond to the same training and test splits used for the fMRI data. The number of images within the training and test splits vary between subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62efabc",
   "metadata": {},
   "source": [
    "### Load stimulus imgaes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30696992",
   "metadata": {},
   "source": [
    "## Visualize the fMRI responses to selected training images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aee5b5",
   "metadata": {},
   "source": [
    "### Map all vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea76e4",
   "metadata": {},
   "source": [
    "#### Select Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53206401",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 42 #'0-[9841, 9841, 9082, 8779, 9841, 9082, 9841, 8779]' for each subject.\n",
    "hemisphere = 'left' # 'left', right'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73441dc6",
   "metadata": {},
   "source": [
    "#### Brain Surface Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307419da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "img_dir = os.path.join(train_img_dir, train_img_list[img])\n",
    "train_img = Image.open(img_dir).convert('RGB')\n",
    "\n",
    "# Plot the image\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(train_img)\n",
    "plt.title('Training image: ' + str(img+1));\n",
    "\n",
    "\n",
    "# Load the brain surface map of all vertices\n",
    "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0]+'h.all-vertices_fsaverage_space.npy')\n",
    "fsaverage_all_vertices = np.load(roi_dir)\n",
    "\n",
    "# Map the fMRI data onto the brain surface map\n",
    "fsaverage_response = np.zeros(len(fsaverage_all_vertices))\n",
    "if hemisphere == 'left':\n",
    "    fsaverage_response[np.where(fsaverage_all_vertices)[0]] = lh_fmri[img]\n",
    "elif hemisphere == 'right':\n",
    "    fsaverage_response[np.where(fsaverage_all_vertices)[0]] = rh_fmri[img]\n",
    "\n",
    "# Create the interactive brain surface map\n",
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "view = plotting.view_surf(\n",
    "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "    surf_map=fsaverage_response,\n",
    "    bg_map=fsaverage['sulc_'+hemisphere],\n",
    "    threshold=1e-14,\n",
    "    cmap='cold_hot',\n",
    "    colorbar=True,\n",
    "    title='All vertices, '+hemisphere+' hemisphere'\n",
    "    )\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcbf676",
   "metadata": {},
   "source": [
    "### Map for chosen individual ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50acac",
   "metadata": {},
   "source": [
    "#### Select ROI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fc378",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = \"lateral\" #\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776223e",
   "metadata": {},
   "source": [
    "#### Brain Surface Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the image\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(train_img)\n",
    "plt.title('Training image: ' + str(img+1));\n",
    "\n",
    "# Define the ROI class based on the selected ROI\n",
    "if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
    "    roi_class = 'prf-visualrois'\n",
    "elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
    "    roi_class = 'floc-bodies'\n",
    "elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
    "    roi_class = 'floc-faces'\n",
    "elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
    "    roi_class = 'floc-places'\n",
    "elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
    "    roi_class = 'floc-words'\n",
    "elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
    "    roi_class = 'streams'\n",
    "\n",
    "# Load the ROI brain surface maps\n",
    "challenge_roi_class_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0]+'h.'+roi_class+'_challenge_space.npy')\n",
    "fsaverage_roi_class_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0]+'h.'+roi_class+'_fsaverage_space.npy')\n",
    "roi_map_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    'mapping_'+roi_class+'.npy')\n",
    "challenge_roi_class = np.load(challenge_roi_class_dir)\n",
    "fsaverage_roi_class = np.load(fsaverage_roi_class_dir)\n",
    "roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
    "\n",
    "# Select the vertices corresponding to the ROI of interest\n",
    "roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
    "challenge_roi = np.asarray(challenge_roi_class == roi_mapping, dtype=int)\n",
    "fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n",
    "\n",
    "# Map the fMRI data onto the brain surface map\n",
    "fsaverage_response = np.zeros(len(fsaverage_roi))\n",
    "if hemisphere == 'left':\n",
    "    fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
    "        lh_fmri[img,np.where(challenge_roi)[0]]\n",
    "elif hemisphere == 'right':\n",
    "    fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
    "        rh_fmri[img,np.where(challenge_roi)[0]]\n",
    "\n",
    "# Create the interactive brain surface map\n",
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "view = plotting.view_surf(\n",
    "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "    surf_map=fsaverage_response,\n",
    "    bg_map=fsaverage['sulc_'+hemisphere],\n",
    "    threshold=1e-14,\n",
    "    cmap='cold_hot',\n",
    "    colorbar=True,\n",
    "    title=roi+', '+hemisphere+' hemisphere'\n",
    "    )\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a38cc",
   "metadata": {},
   "source": [
    "## Building the Linearizing Encoding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea6f64",
   "metadata": {},
   "source": [
    "Taking the training data that we have loaded and visualized, we can now begin pre-processing it before training and fitting our regression models on it.\n",
    "\n",
    "We first need to split the available data into training and validation partitions to allow for model performance evaluation after fitting our models. Normally we would use one of sklearn's cross-validation method implementations that provide built in functionality for splitting the data before training and validating the model. However, our training data consists of both stimulus images and their corresponding fMRI responses, also requiring unique pre-processing, (extracting and downsampling image features). Therefore we define a series of functions to; pre-process the images; create a data structure to store them;   randomly select a given proportion (10%) of image indicies for validation, and use the selected indexes to retrieve the relevant fMRI data, before repeating with a different random seed and therefore image indicies for each fold. We then average the evaluation (correlation) scores from each fold to give a final cross-validated average score for each hemisphere (LH, RH)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf53df2",
   "metadata": {},
   "source": [
    "We first define the transform to be applied to the images prior to inputting them to AlexNet. We use a standard preprocessing pipeline, synonymous with typical/common computer vision preprocessing, for formatting the images for optimal feature extraction by the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), # resize the images to 224x24 pixels\n",
    "    transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c53b2e",
   "metadata": {},
   "source": [
    "Next we define a custom implementation of the ImageDataset class from PyTorch, for storing the images from each partition before passing them to AlexNet, where each image is loaded and pretranformed before being returned to be used for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d879013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu')\n",
    "        if self.transform:\n",
    "            img = self.transform(img).to(device)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee5399",
   "metadata": {},
   "source": [
    "We then load the pre-trained AlexNet model, selecting the feature extraction layer which we wish to use for training our model on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet')\n",
    "model.to(device) # send the model to the chosen device ('cpu')\n",
    "model.eval() # set the model to evaluation mode, since we are not training it\n",
    "\n",
    "#train_nodes, _ = get_graph_node_names(model)\n",
    "#print(train_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61457ef9",
   "metadata": {},
   "source": [
    "#### Select feature layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layer = \"features.2\" #[\"features.2\", \"features.5\", \"features.7\", \"features.9\", \"features.12\", \"classifier.2\", \"classifier.5\", \"classifier.6\"] {allow-input: true}\n",
    "feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c4ad0",
   "metadata": {},
   "source": [
    "The feature layer from AlexNet is very large, so we downsample the output using the dimensionality reduction technique, Principal Component Analysis (PCA). The below method is used to fit the PCA on the training images features, and then downsample the training, validation and test images features.\n",
    "\n",
    "'IncrementalPCA()' is used to partially fit the pca on the images in batches, saving memory as jupyter notebook has RAM limitations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ae24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=100, batch_size=300)\n",
    "\n",
    "    # Fit PCA to batch\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Fit PCA to batch\n",
    "        pca.partial_fit(ft.detach().cpu().numpy())\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c3505",
   "metadata": {},
   "source": [
    "extract_features() then takes the outputs of the above methods and cells as parameters, (feature_extractor for extracting image features from chosen 'model_layer', 'dataloader' containing 'ImageDataset' for extraction, and 'pca' for downsampling extracted features), returning the downsampled imaged features from AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb361ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(feature_extractor, dataloader, pca):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Apply PCA transform\n",
    "        ft = pca.transform(ft.cpu().detach().numpy())\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4c8cb",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965be40",
   "metadata": {},
   "source": [
    "We now perform the cross-validation. This involves setting (incrementing) the random seed and then splitting the training data. The images corresponding to the indexes selected for each partition are then loaded into the relevant ImageDataset using the DataLoader class (also from PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c69363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model_parameters, folds):\n",
    "    lh_correlation_arrays = []\n",
    "    rh_correlation_arrays = []\n",
    "\n",
    "    for i in range (folds):\n",
    "        rand_seed = i\n",
    "        np.random.seed(rand_seed)\n",
    "\n",
    "        # Calculate how many stimulus images correspond to 90% of the training data\n",
    "        num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "        # Shuffle all training stimulus images\n",
    "        idxs = np.arange(len(train_img_list))\n",
    "        np.random.shuffle(idxs)\n",
    "        # Assign 90% of the shuffled stimulus images to the training partition,\n",
    "        # and 10% to the validation partition\n",
    "        idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "\n",
    "        print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "        print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "\n",
    "        batch_size = 300\n",
    "        # Get the paths of all image files\n",
    "        train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "        test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "        # The DataLoaders contain the ImageDataset class\n",
    "        train_imgs_dataloader = DataLoader(\n",
    "            ImageDataset(train_imgs_paths, idxs_train, transform), \n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        val_imgs_dataloader = DataLoader(\n",
    "            ImageDataset(train_imgs_paths, idxs_val, transform), \n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        lh_fmri_train = lh_fmri[idxs_train]\n",
    "        lh_fmri_val = lh_fmri[idxs_val]\n",
    "        rh_fmri_train = rh_fmri[idxs_train]\n",
    "        rh_fmri_val = rh_fmri[idxs_val]\n",
    "\n",
    "        pca = fit_pca(feature_extractor, train_imgs_dataloader)\n",
    "\n",
    "        features_train = extract_features(feature_extractor, train_imgs_dataloader, pca)\n",
    "        features_val = extract_features(feature_extractor, val_imgs_dataloader, pca)\n",
    "\n",
    "        print('\\nTraining images features:')\n",
    "        print(features_train.shape)\n",
    "        print('(Training stimulus images × PCA features)')\n",
    "\n",
    "        print('\\nValidation images features:')\n",
    "        print(features_val.shape)\n",
    "        print('(Validation stimulus images × PCA features)')\n",
    "\n",
    "        # Fit linear regressions on the training data\n",
    "        if(model_parameters[0] == \"default\"):\n",
    "            reg_lh = LinearRegression().fit(features_train, lh_fmri_train)\n",
    "            reg_rh = LinearRegression().fit(features_train, rh_fmri_train)\n",
    "        elif(model_parameters[0] == \"Lasso\"):\n",
    "            reg_lh = Lasso(alpha = model_parameters[1]).fit(features_train, lh_fmri_train)\n",
    "            reg_rh = Lasso(alpha = model_parameters[1]).fit(features_train, rh_fmri_train)\n",
    "        elif(model_parameters[0] == \"Ridge\"):\n",
    "            reg_lh = Ridge(alpha = model_parameters[1]).fit(features_train, lh_fmri_train)\n",
    "            reg_lh = Ridge(alpha = model_parameters[1]).fit(features_train, rh_fmri_train)\n",
    "        # Use fitted linear regressions to predict the validation fMRI data\n",
    "        lh_fmri_val_pred = reg_lh.predict(features_val)\n",
    "        rh_fmri_val_pred = reg_rh.predict(features_val)\n",
    "\n",
    "        # Empty correlation array of shape: (LH vertices)\n",
    "        lh_correlation = np.zeros(lh_fmri_val_pred.shape[1])\n",
    "        # Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "        for v in tqdm(range(lh_fmri_val_pred.shape[1])):\n",
    "            lh_correlation[v] = corr(lh_fmri_val_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "        lh_correlation_arrays.append(lh_correlation)\n",
    "\n",
    "        # Empty correlation array of shape: (RH vertices)\n",
    "        rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n",
    "        # Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "        for v in tqdm(range(rh_fmri_val_pred.shape[1])):\n",
    "            rh_correlation[v] = corr(rh_fmri_val_pred[:,v], rh_fmri_val[:,v])[0]\n",
    "        rh_correlation_arrays.append(rh_correlation)\n",
    "\n",
    "    lh_average_correlation = []    \n",
    "    rh_average_correlation = []\n",
    "\n",
    "    for i in range(len(lh_correlation_arrays[0])):\n",
    "        lh_num = 0\n",
    "        rh_num = 0\n",
    "        for j in range (len(lh_correlation_arrays)):\n",
    "            lh_num += lh_correlation_arrays[j][i];\n",
    "            rh_num += rh_correlation_arrays[j][i];\n",
    "        lh_average_correlation.append(lh_num / len(lh_correlation_arrays));\n",
    "        rh_average_correlation.append(rh_num / len(rh_correlation_arrays));\n",
    "        \n",
    "    return lh_average_correlation, rh_average_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02addc57",
   "metadata": {},
   "source": [
    "For standard 'cross_validation()' with default parameters, each fold takes ~5 mins to fully execute. This is due to the overhead of extracting features and fitting the PCA on the current train-validation splits every fold. While this improves the accuracy of the image features used for training, it significantly slows down runtime.\n",
    "\n",
    "Therefore we implement a 'fast_cross_validation()' method used for hyperparameter tuning, where we extract and downsample features for all images prior, (storing the full image 'features' set as a global variable). Each fold then simply randomly selects the 'idxs' (using a new random seed) and uses them to partition the 'features' into 'features_train' and 'features_val', and each hemisphere's, '*h_fmri' vertex responses into '*h_fmri_train' and '*h_fmri_val'.\\\n",
    "We then just repeat the same process of fitting the model (using 'model_parameters') and predicting fMRI responses, evaluating the predictions agasint the unseen ground truth fMRI data from the validation partition. We then return an array of the average correlation scores for each vertex across all the cross validation folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55303561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_cross_validation(model_parameters, folds):\n",
    "    num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "    \n",
    "    lh_correlation_arrays = []\n",
    "    rh_correlation_arrays = []\n",
    "    \n",
    "    for i in tqdm(range(folds)):\n",
    "        rand_seed = i\n",
    "        np.random.seed(rand_seed)\n",
    "        \n",
    "        idxs = np.arange(len(train_img_list))\n",
    "        np.random.shuffle(idxs)\n",
    "        idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "\n",
    "        features_train = features[idxs_train] \n",
    "        features_val = features[idxs_val]\n",
    "\n",
    "        lh_fmri_train = lh_fmri[idxs_train]\n",
    "        lh_fmri_val = lh_fmri[idxs_val]\n",
    "        rh_fmri_train = rh_fmri[idxs_train]\n",
    "        rh_fmri_val = rh_fmri[idxs_val]\n",
    "\n",
    "        # Fit linear regressions on the training data\n",
    "        if(model_parameters[0] == \"default\"):\n",
    "            reg_lh = LinearRegression().fit(features_train, lh_fmri_train)\n",
    "            reg_rh = LinearRegression().fit(features_train, rh_fmri_train)\n",
    "        elif(model_parameters[0] == \"Lasso\"):\n",
    "            reg_lh = Lasso(alpha = model_parameters[1]).fit(features_train, lh_fmri_train)\n",
    "            reg_rh = Lasso(alpha = model_parameters[1]).fit(features_train, rh_fmri_train)\n",
    "        elif(model_parameters[0] == \"Ridge\"):\n",
    "            reg_lh = Ridge(alpha = 1 - model_parameters[1]).fit(features_train, lh_fmri_train)\n",
    "            reg_rh = Ridge(alpha = 1 - model_parameters[1]).fit(features_train, rh_fmri_train)\n",
    "            # Use fitted linear regressions to predict the validation fMRI data\n",
    "        lh_fmri_val_pred = reg_lh.predict(features_val)\n",
    "        rh_fmri_val_pred = reg_rh.predict(features_val)\n",
    "\n",
    "        # Empty correlation array of shape: (LH vertices)\n",
    "        lh_correlation = np.zeros(lh_fmri_val_pred.shape[1])\n",
    "        # Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "        for v in range(lh_fmri_val_pred.shape[1]):\n",
    "            c = corr(lh_fmri_val_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "            if math.isnan(c) == True:\n",
    "                lh_correlation[v] = 0\n",
    "            else:\n",
    "                lh_correlation[v] = c\n",
    "        lh_correlation_arrays.append(lh_correlation)\n",
    "\n",
    "        # Empty correlation array of shape: (RH vertices)\n",
    "        rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n",
    "        # Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "        for v in range(rh_fmri_val_pred.shape[1]):\n",
    "            c = corr(rh_fmri_val_pred[:,v], rh_fmri_val[:,v])[0]\n",
    "            if math.isnan(c) == True:\n",
    "                rh_correlation[v] = 0\n",
    "            else:\n",
    "                rh_correlation[v] = c\n",
    "        rh_correlation_arrays.append(rh_correlation)\n",
    "\n",
    "    lh_average_correlation = []    \n",
    "    rh_average_correlation = []\n",
    "\n",
    "    for i in range(len(lh_correlation_arrays[0])):\n",
    "        lh_num = 0\n",
    "        rh_num = 0\n",
    "        for j in range (len(lh_correlation_arrays)):\n",
    "            lh_num += lh_correlation_arrays[j][i];\n",
    "            rh_num += rh_correlation_arrays[j][i];\n",
    "        lh_average_correlation.append(lh_num / len(lh_correlation_arrays));\n",
    "        rh_average_correlation.append(rh_num / len(rh_correlation_arrays));\n",
    "        \n",
    "    return lh_average_correlation, rh_average_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3af2b9",
   "metadata": {},
   "source": [
    "cross_val_scores() calls either full or fast cross_validation() and calculates and outputs the median noise-normalized correlation (MNNC) score using the returned arrays of the average correlation scores for each vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2246490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_scores(fast, params, folds):\n",
    "    if(fast == True):\n",
    "        lh_score, rh_score = fast_cross_validation(params,folds)\n",
    "    else:\n",
    "        lh_score, rh_score = cross_validation(params,folds)\n",
    "\n",
    "    print('\\nLH correlation for all verticies: ')\n",
    "    print(\" [ \", lh_score[1], \",\", lh_score[2], \",\", lh_score[3], \", ... ]\")\n",
    "\n",
    "    result = []\n",
    "    for i in range (0,len(lh_score)):\n",
    "        result.append((lh_score[i]*lh_score[i])/0.36)   \n",
    "    print(\"LH median noise-normalized score: \")\n",
    "    print(np.median(result))\n",
    "\n",
    "    print('\\nRH correlation for all verticies: ')\n",
    "    print(\" [ \", rh_score[1], \",\", rh_score[2], \",\", rh_score[3], \", ... ]\")\n",
    "\n",
    "    result = []\n",
    "    for i in range (0,len(rh_score)):\n",
    "        result.append((rh_score[i]*rh_score[i])/0.36)\n",
    "\n",
    "    print(\"RH median noise-normalized score: \")\n",
    "    print(np.median(result))\n",
    "    \n",
    "    return lh_score, rh_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df9c54",
   "metadata": {},
   "source": [
    "#### Select parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dce791",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast = True # True, False\n",
    "folds = 3 # 3, 5, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248f6a5",
   "metadata": {},
   "source": [
    "#### Cross-validate default linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b55028",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"Ridge\",0.05]\n",
    "lh_score, rh_score = cross_val_scores(fast, params, folds)\n",
    "params = [\"Ridge\",0.02]\n",
    "lh_score, rh_score = cross_val_scores(fast, params, folds)\n",
    "params = [\"Ridge\",0.03]\n",
    "lh_score, rh_score = cross_val_scores(fast, params, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fcfd92",
   "metadata": {},
   "source": [
    "### Visualize averaged correlation score from cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8f84b",
   "metadata": {},
   "source": [
    "We map the average correlations for each vertex (lh_score and rh_score), outputted from cross-validating our default Linear Regression model against 10 folds with different train-validation splits, to the brain surface map in fsaverage space. This is the same process as used for mapping and visualizing the fMRI data, with spatial heatmaps used to represent individual vertex's encoding accuracies across the visual cortex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43d437",
   "metadata": {},
   "source": [
    "#### Select parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere = 'left' #@param ['left', 'right'] {allow-input: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89acc2f",
   "metadata": {},
   "source": [
    "#### Brain Surface Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c938334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the brain surface map of all vertices\n",
    "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0]+'h.all-vertices_fsaverage_space.npy')\n",
    "fsaverage_all_vertices = np.load(roi_dir)\n",
    "\n",
    "# Map the correlation results onto the brain surface map\n",
    "fsaverage_correlation = np.zeros(len(fsaverage_all_vertices))\n",
    "if hemisphere == 'left':\n",
    "    fsaverage_correlation[np.where(fsaverage_all_vertices)[0]] = lh_score\n",
    "elif hemisphere == 'right':\n",
    "    fsaverage_correlation[np.where(fsaverage_all_vertices)[0]] = rh_score\n",
    "\n",
    "# Create the interactive brain surface map\n",
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "view = plotting.view_surf(\n",
    "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "    surf_map=fsaverage_correlation,\n",
    "    bg_map=fsaverage['sulc_'+hemisphere],\n",
    "    threshold=1e-14,\n",
    "    cmap='cold_hot',\n",
    "    colorbar=True,\n",
    "    title='Encoding accuracy, '+hemisphere+' hemisphere'\n",
    "    )\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2a23a",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ef70c",
   "metadata": {},
   "source": [
    "By utilising the implemented fast_cross_validation method above, we can test model encoding accuracy for a variety of different models and parameters and compare their median noise-normalized correlation (MNNC) score.\n",
    "\n",
    "As explained, we first must extract and downsample the image features of all the available training images prior to cross-validating and hyperparameter tuning our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54facef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_train = np.arange(len(train_img_list))\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "batch_size = 300    \n",
    "    \n",
    "all_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(train_imgs_paths, idxs_train, transform),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "pca = fit_pca(feature_extractor, all_imgs_dataloader)\n",
    "\n",
    "features = extract_features(feature_extractor, all_imgs_dataloader, pca)\n",
    "\n",
    "print('\\nAll images features:')\n",
    "print(features.shape)\n",
    "print('(Training stimulus images × PCA features)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b36db35",
   "metadata": {},
   "source": [
    "hyperparameter_tuning() then makes the calls to fast_cross_validation, slightly altering the hyperparameters used for training the model each time. We then take the models with maximum lh and rh MNNC scores and output them along with the parameters used. Linear Regression doesn't have any tuneable hyperparameters, but Lasso and Ridge regression have a regularization, alpha parameter, so we test and tune these linear regression variant models.\n",
    "\n",
    "We find (as expected) L1 typically performs better closer to 0, while L2 performs better closer to 1. Therefore when selecting parameters, we start from '0+increment' and increment up for Lasso and from 1 and decrement down for Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266bac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(variant, increment, folds, num_models):\n",
    "    lh_scores = []\n",
    "    rh_scores = []\n",
    "\n",
    "    for i in tqdm(range(num_models)):\n",
    "        if variant == \"Lasso\":\n",
    "            params = [variant, (i+1)*increment]\n",
    "        elif variant == \"Ridge\":\n",
    "            params = [variant, i*increment]\n",
    "        else:\n",
    "            print(\"Error: Invalid 'variant'\")\n",
    "            break\n",
    "        lh_score, rh_score = fast_cross_validation(params,folds)\n",
    "        \n",
    "        result = []\n",
    "        for i in range (0,len(lh_score)):\n",
    "            result.append((lh_score[i]*lh_score[i])/0.36)\n",
    "        lh_scores.append(np.median(result))\n",
    "        \n",
    "        result = []\n",
    "        for i in range (0,len(rh_score)):\n",
    "            result.append((rh_score[i]*rh_score[i])/0.36)\n",
    "        rh_scores.append(np.median(result))\n",
    "    \n",
    "    if variant == \"Lasso\":\n",
    "        best_lh_param = ((lh_scores.index(max(lh_scores)) + 1) * increment)\n",
    "        best_lh_score = max(lh_scores)\n",
    "        best_rh_param = ((rh_scores.index(max(rh_scores)) + 1) * increment)\n",
    "        best_rh_score = max(rh_scores)\n",
    "    elif variant == \"Ridge\":\n",
    "        best_lh_param = 1 - ((lh_scores.index(max(lh_scores))) * increment)\n",
    "        best_lh_score = max(lh_scores)\n",
    "        best_rh_param = 1 - ((rh_scores.index(max(rh_scores))) * increment)\n",
    "        best_rh_score = max(rh_scores)\n",
    "        \n",
    "    return best_lh_param, best_lh_score, best_rh_param, best_rh_score, lh_scores, rh_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba06f62",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7795ab",
   "metadata": {},
   "source": [
    "We first tested both lasso and ridge regression with a broad range of regularization weights (0.05 - 1) and moderately large step size (0.05), using only a small number of folds. [folds = 3, increment = 0.05, num_models = 20] \\\n",
    "This resulted in:\n",
    "\n",
    "    Lasso regression:\n",
    "    Best Parameter (lh): 0.05\n",
    "    Score:  0.13186117333759984\n",
    "    Best Parameter (rh): 0.05\n",
    "    Score:  0.12192197343991548\n",
    "    \n",
    "    Ridge regression: \n",
    "    Best Parameter (lh):  1.0\n",
    "    Score:  0.1283576704722752\n",
    "    Best Parameter (rh):  1.0\n",
    "    Score:  0.1179841435515414\n",
    "    \n",
    "The below cell then shows the focused hyperparameter optimization. \\\n",
    "(NOTE: runtime ~9 hours! Therefore code has been commented out to allow running the entire notebook in a reasonable timeframe.)\n",
    "\n",
    "(Sample output shown below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c2fbb",
   "metadata": {},
   "source": [
    "#### Select Parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd749438",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 10\n",
    "increment = 0.005\n",
    "num_models = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_param, lh_score, rh_param, rh_score, lh_scores, rh_scores = hyperparameter_tuning(\"Lasso\",increment,folds,num_models)\n",
    "print(\"Lasso regression: \")\n",
    "print(\"Best Parameter (lh): \", lh_param)\n",
    "print(\"Score: \", lh_score)\n",
    "print(\"Best Parameter (rh): \", rh_param)\n",
    "print(\"Score: \", rh_score)\n",
    "\n",
    "lh_param, lh_score, rh_param, rh_score, lh_scores, rh_scores = hyperparameter_tuning(\"Ridge\",increment,folds,num_models)\n",
    "print(\"\\nRidge regression: \")\n",
    "print(\"Best Parameter (lh): \", lh_param)\n",
    "print(\"Score: \", lh_score)\n",
    "print(\"Best Parameter (rh): \", rh_param)\n",
    "print(\"Score: \", rh_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9491ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lh_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb82a57",
   "metadata": {},
   "source": [
    "100%|██████████████████████████████████████████████████████████████████| 10/10 [           ]\\\n",
    "100%|██████████████████████████████████████████████████████████████████| 16/16 [             ]\n",
    "\n",
    "Lasso regression:\n",
    "\n",
    "Best Parameter (lh):  \n",
    "Score:  \n",
    "\n",
    "Best Parameter (rh):  \n",
    "Score:  \n",
    "\n",
    "...\n",
    "\n",
    "100%|██████████████████████████████████████████████████████████████████| 10/10 [           ] \\\n",
    "100%|██████████████████████████████████████████████████████████████████| 16/16 [             ]\n",
    "\n",
    "Ridge regression:\n",
    "\n",
    "Best Parameter (lh):  \n",
    "Score:  \n",
    "\n",
    "Best Parameter (rh):  \n",
    "Score:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c361f",
   "metadata": {},
   "source": [
    "#### Optimally performing model: Lasso\n",
    "\n",
    "#### Parameters: L1 (alpha) regularizer = \n",
    "Estimated MNNC Score = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa04745",
   "metadata": {},
   "source": [
    "### Visualize final model encoding accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831d9f0",
   "metadata": {},
   "source": [
    "Now that we have validated our model and found the optimal hyperparamters, we have an estimate of our best and final model's expected performance for predicting the unseen test data. We can visualize this evaluation by mapping the average correlation scores, obtained from cross-validation, for each vertex in fsaverage space, comparing the model with default parameters against our final model with tuned hyperparameters. We also show each individual ROI encoding accuracy, allowing visual (side-by-side) comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03c8e8",
   "metadata": {},
   "source": [
    "#### Select Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere = 'left' #'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c6d5e",
   "metadata": {},
   "source": [
    "#### Brain Surface Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the brain surface map of all vertices\n",
    "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0]+'h.all-vertices_fsaverage_space.npy')\n",
    "fsaverage_all_vertices = np.load(roi_dir)\n",
    "\n",
    "# Map the correlation results onto the brain surface map\n",
    "fsaverage_correlation = np.zeros(len(fsaverage_all_vertices))\n",
    "if hemisphere == 'left':\n",
    "    fsaverage_correlation[np.where(fsaverage_all_vertices)[0]] = lh_score\n",
    "elif hemisphere == 'right':\n",
    "    fsaverage_correlation[np.where(fsaverage_all_vertices)[0]] = rh_score\n",
    "\n",
    "# Create the interactive brain surface map\n",
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "view = plotting.view_surf(\n",
    "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "    surf_map=fsaverage_correlation,\n",
    "    bg_map=fsaverage['sulc_'+hemisphere],\n",
    "    threshold=1e-14,\n",
    "    cmap='cold_hot',\n",
    "    colorbar=True,\n",
    "    title='Encoding accuracy, '+hemisphere+' hemisphere'\n",
    "    )\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd5d266",
   "metadata": {},
   "source": [
    "#### ROI Bar Chart Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ROI classes mapping dictionaries\n",
    "roi_mapping_files = ['mapping_prf-visualrois.npy', 'mapping_floc-bodies.npy',\n",
    "    'mapping_floc-faces.npy', 'mapping_floc-places.npy',\n",
    "    'mapping_floc-words.npy', 'mapping_streams.npy']\n",
    "roi_name_maps = []\n",
    "for r in roi_mapping_files:\n",
    "    roi_name_maps.append(np.load(os.path.join(args.data_dir, 'roi_masks', r),\n",
    "        allow_pickle=True).item())\n",
    "\n",
    "# Load the ROI brain surface maps\n",
    "lh_challenge_roi_files = ['lh.prf-visualrois_challenge_space.npy',\n",
    "    'lh.floc-bodies_challenge_space.npy', 'lh.floc-faces_challenge_space.npy',\n",
    "    'lh.floc-places_challenge_space.npy', 'lh.floc-words_challenge_space.npy',\n",
    "    'lh.streams_challenge_space.npy']\n",
    "rh_challenge_roi_files = ['rh.prf-visualrois_challenge_space.npy',\n",
    "    'rh.floc-bodies_challenge_space.npy', 'rh.floc-faces_challenge_space.npy',\n",
    "    'rh.floc-places_challenge_space.npy', 'rh.floc-words_challenge_space.npy',\n",
    "    'rh.streams_challenge_space.npy']\n",
    "lh_challenge_rois = []\n",
    "rh_challenge_rois = []\n",
    "for r in range(len(lh_challenge_roi_files)):\n",
    "    lh_challenge_rois.append(np.load(os.path.join(args.data_dir, 'roi_masks',\n",
    "        lh_challenge_roi_files[r])))\n",
    "    rh_challenge_rois.append(np.load(os.path.join(args.data_dir, 'roi_masks',\n",
    "        rh_challenge_roi_files[r])))\n",
    "\n",
    "# Select the correlation results vertices of each ROI\n",
    "roi_names = []\n",
    "lh_roi_correlation = []\n",
    "rh_roi_correlation = []\n",
    "for r1 in range(len(lh_challenge_rois)):\n",
    "    for r2 in roi_name_maps[r1].items():\n",
    "        if r2[0] != 0: # zeros indicate to vertices falling outside the ROI of interest\n",
    "            roi_names.append(r2[1])\n",
    "            lh_roi_idx = np.where(lh_challenge_rois[r1] == r2[0])[0]\n",
    "            rh_roi_idx = np.where(rh_challenge_rois[r1] == r2[0])[0]\n",
    "            lh_roi_correlation.append(lh_correlation[lh_roi_idx])\n",
    "            rh_roi_correlation.append(rh_correlation[rh_roi_idx])\n",
    "roi_names.append('All vertices')\n",
    "lh_roi_correlation.append(lh_correlation)\n",
    "rh_roi_correlation.append(rh_correlation)\n",
    "\n",
    "# Create the plot\n",
    "lh_median_roi_correlation = [np.median(lh_roi_correlation[r])\n",
    "    for r in range(len(lh_roi_correlation))]\n",
    "rh_median_roi_correlation = [np.median(rh_roi_correlation[r])\n",
    "    for r in range(len(rh_roi_correlation))]\n",
    "plt.figure(figsize=(18,6))\n",
    "x = np.arange(len(roi_names))\n",
    "width = 0.30\n",
    "plt.bar(x - width/2, lh_median_roi_correlation, width, label='Left Hemisphere')\n",
    "plt.bar(x + width/2, rh_median_roi_correlation, width,\n",
    "    label='Right Hemishpere')\n",
    "plt.xlim(left=min(x)-.5, right=max(x)+.5)\n",
    "plt.ylim(bottom=0, top=1)\n",
    "plt.xlabel('ROIs')\n",
    "plt.xticks(ticks=x, labels=roi_names, rotation=60)\n",
    "plt.ylabel('Median Pearson\\'s $r$')\n",
    "plt.legend(frameon=True, loc=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfb93c8",
   "metadata": {},
   "source": [
    "## Challenge submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2688c66",
   "metadata": {},
   "source": [
    "Finally, we train and fit the final model on all the available training data, originally supplied by the NSD dataset. This is the our best possible, validated, linear regression model, with optimised parameters and maximally trained feature weights for predicting fMRI repsonses to unseen visual stimuli. \n",
    "\n",
    "We then use our final model to output fMRI predictions for all verticies across all brain ROIs for all 8 subjects, before saving and submitting them through the offical Algonauts submission link. We are then given a ranking and challenge (MNNC) score for our model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_train = np.arange(len(train_img_list))\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "train_imgs_dataloader = DataLoader(\n",
    "        ImageDataset(train_imgs_paths, idxs_train, transform), \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "test_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(test_imgs_paths, idxs_test, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "lh_fmri_train = lh_fmri[idxs_train]\n",
    "rh_fmri_train = rh_fmri[idxs_train]\n",
    "\n",
    "del lh_fmri, rh_fmri\n",
    "\n",
    "pca = fit_pca(feature_extractor, train_imgs_dataloader)\n",
    "\n",
    "features_train = extract_features(feature_extractor, train_imgs_dataloader, pca)\n",
    "features_test = extract_features(feature_extractor, test_imgs_dataloader, pca)\n",
    "\n",
    "del model, pca\n",
    "\n",
    "reg_lh = LinearRegression().fit(features_train, lh_fmri_train)\n",
    "reg_rh = LinearRegression().fit(features_train, rh_fmri_train)\n",
    "\n",
    "lh_fmri_test_pred = reg_lh.predict(features_test)\n",
    "rh_fmri_test_pred = reg_rh.predict(features_test)\n",
    "\n",
    "lh_fmri_test_pred = lh_fmri_test_pred.astype(np.float32)\n",
    "rh_fmri_test_pred = rh_fmri_test_pred.astype(np.float32)\n",
    "\n",
    "np.save(os.path.join(args.subject_submission_dir, 'lh_pred_test.npy'), lh_fmri_test_pred)\n",
    "np.save(os.path.join(args.subject_submission_dir, 'rh_pred_test.npy'), rh_fmri_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159f363",
   "metadata": {},
   "source": [
    "Rank:\n",
    "\n",
    "Score: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:algonauts]",
   "language": "python",
   "name": "conda-env-algonauts-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
