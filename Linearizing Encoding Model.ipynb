{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df20a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236f8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'nsd_data'\n",
    "parent_submission_dir = 'test_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' #'cuda'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35fbb8",
   "metadata": {},
   "source": [
    "### Select Subject (1-8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bee6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 1 #2 3 4 5 6 7 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e65b15",
   "metadata": {},
   "source": [
    "### Define data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "    \n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "    # Create the submission directory if not existing\n",
    "    if not os.path.isdir(self.subject_submission_dir):\n",
    "        os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54097bbd",
   "metadata": {},
   "source": [
    "### Load fMRI training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7120896",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fdb2d2",
   "metadata": {},
   "source": [
    "## fMRI ROI indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7114b",
   "metadata": {},
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde3c71",
   "metadata": {},
   "source": [
    "### Visualize all vertices on a brain surface map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8146d",
   "metadata": {},
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b30e3",
   "metadata": {},
   "source": [
    "#### Select Hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dce5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere = 'left' #'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67484efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the brain surface map of all vertices\n",
    "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0]+'h.all-vertices_fsaverage_space.npy')\n",
    "fsaverage_all_vertices = np.load(roi_dir)\n",
    "\n",
    "# Create the interactive brain surface map\n",
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "view = plotting.view_surf(\n",
    "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "    surf_map=fsaverage_all_vertices,\n",
    "    bg_map=fsaverage['sulc_'+hemisphere],\n",
    "    threshold=1e-14,\n",
    "    cmap='cool',\n",
    "    colorbar=False,\n",
    "    title='All vertices, '+hemisphere+' hemisphere'\n",
    "    )\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e5830",
   "metadata": {},
   "source": [
    "### Visualize a chosen ROI on a brain surface map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f494197",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere = 'left' #@param ['left', 'right'] {allow-input: true}\n",
    "roi = \"EBA\" #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
    "\n",
    "# Define the ROI class based on the selected ROI\n",
    "if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
    "    roi_class = 'prf-visualrois'\n",
    "elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
    "    roi_class = 'floc-bodies'\n",
    "elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
    "    roi_class = 'floc-faces'\n",
    "elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
    "    roi_class = 'floc-places'\n",
    "elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
    "    roi_class = 'floc-words'\n",
    "elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
    "    roi_class = 'streams'\n",
    "\n",
    "# Load the ROI brain surface maps\n",
    "roi_class_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    hemisphere[0]+'h.'+roi_class+'_fsaverage_space.npy')\n",
    "roi_map_dir = os.path.join(args.data_dir, 'roi_masks',\n",
    "    'mapping_'+roi_class+'.npy')\n",
    "fsaverage_roi_class = np.load(roi_class_dir)\n",
    "roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
    "\n",
    "# Select the vertices corresponding to the ROI of interest\n",
    "roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
    "fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n",
    "\n",
    "# Create the interactive brain surface map\n",
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "view = plotting.view_surf(\n",
    "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
    "    surf_map=fsaverage_roi,\n",
    "    bg_map=fsaverage['sulc_'+hemisphere],\n",
    "    threshold=1e-14,\n",
    "    cmap='cool',\n",
    "    colorbar=False,\n",
    "    title=roi+', '+hemisphere+' hemisphere'\n",
    "    )\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893f6f1",
   "metadata": {},
   "source": [
    "### Stimulus images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6680299",
   "metadata": {},
   "source": [
    "All images consist of natural scenes coming from the COCO dataset.\n",
    "\n",
    "The images are divided into a training and a test split (corresponding to the fMRI training and test data splits). The amount of training and test images varies between subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:algonauts]",
   "language": "python",
   "name": "conda-env-algonauts-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
